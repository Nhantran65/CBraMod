# CBraMod ADHD Fine-tuning with K-Fold Cross Validation

HÆ°á»›ng dáº«n chi tiáº¿t Ä‘á»ƒ cháº¡y fine-tuning CBraMod trÃªn dataset ADHD vá»›i 5-fold cross validation.

## ğŸ“‹ Tá»•ng quan

Notebook `kfold_adhd_finetune.ipynb` implement 5-fold stratified cross-validation Ä‘á»ƒ fine-tune mÃ´ hÃ¬nh CBraMod cho bÃ i toÃ¡n phÃ¢n loáº¡i ADHD vs Control, sá»­ dá»¥ng frozen backbone Ä‘á»ƒ tÄƒng tá»‘c training.

## ğŸ¯ TÃ­nh nÄƒng chÃ­nh

- âœ… **Stratified K-Fold**: Äáº£m báº£o cÃ¢n báº±ng classes (ADHD/Control) trong má»—i fold
- âœ… **Frozen Backbone**: Chá»‰ fine-tune classifier layer cuá»‘i (~3x faster)
- âœ… **Real Labels**: Sá»­ dá»¥ng labels tháº­t tá»« dataset (55.7% ADHD, 44.3% Control)
- âœ… **Flexible Dataset Size**: Chá»n giá»¯a full dataset (2.1M) hoáº·c sample dataset (10K)
- âœ… **Best Model Saving**: Tá»± Ä‘á»™ng save best model cho má»—i fold
- âœ… **Comprehensive Evaluation**: Accuracy, F1-score, Cohen's Kappa
- âœ… **Detailed Visualization**: Training curves, confusion matrices, performance plots

## ğŸš€ CÃ¡ch cháº¡y

### 1. Environment Setup

```bash
# CÃ i Ä‘áº·t dependencies
conda install -c conda-forge einops
pip install torch torchvision torchaudio
pip install scikit-learn pandas matplotlib seaborn tqdm
```

### 2. Dataset Configuration

Má»Ÿ file `kfold_adhd_finetune.ipynb`, tÃ¬m cell thá»© 3 vÃ  chá»‰nh config:

```python
# ğŸ¯ DATASET CONFIGURATION - CHOOSE YOUR OPTION
USE_FULL_DATASET = True   # True: 2.1M samples, False: sample dataset
SAMPLE_SIZE = 10000       # Sá»‘ samples náº¿u dÃ¹ng sample dataset
```

**Options:**

| Config | Dataset Size | Training Time | Performance | Recommended For |
|--------|-------------|---------------|-------------|-----------------|
| `USE_FULL_DATASET = True` | 2,166,383 samples | ~15-20 min | Highest accuracy | Final results |
| `USE_FULL_DATASET = False` | 10,000 samples | ~5-10 min | Good for testing | Quick experiments |

### 3. Cháº¡y Notebook

**Cháº¡y tuáº§n tá»± cÃ¡c cells:**

1. **Cell 1**: Import libraries vÃ  setup seeds
2. **Cell 2**: Load dataset (chá»n full hoáº·c sample)
3. **Cell 3**: Install missing packages (einops)
4. **Cell 4**: Load CBraMod model vÃ  config
5. **Cell 5**: Define training functions
6. **Cell 6**: **MAIN TRAINING** - 5-Fold Cross Validation
7. **Cell 7**: Results analysis vÃ  visualization

### 4. Monitor Training Progress

Trong quÃ¡ trÃ¬nh training, báº¡n sáº½ tháº¥y:

```
ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ FOLD 1/5 ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥
ğŸ“Š Data Split:
  Train: 1,733,106 samples (ADHD: 965,655, Control: 767,451)
  Val:   433,277 samples (ADHD: 241,414, Control: 191,863)

ğŸš€ Training Fold 1
============================================================
Epoch Train Loss  Train Acc   Val Loss    Val Acc     Best  
============================================================
1     0.6234      0.6543       0.5987       0.6123       â­ NEW
2     0.5876      0.6789       0.5654       0.6456       â­ NEW
...
ğŸ† Best Validation Accuracy: 0.6456
ğŸ’¾ Best model saved: ./saved_models/best_model_fold_1.pth
```

## ğŸ“Š Káº¿t quáº£ vÃ  Outputs

### Model Files
```
saved_models/
â”œâ”€â”€ best_model_fold_1.pth          # Best model cho fold 1
â”œâ”€â”€ best_model_fold_2.pth          # Best model cho fold 2
â”œâ”€â”€ best_model_fold_3.pth          # Best model cho fold 3
â”œâ”€â”€ best_model_fold_4.pth          # Best model cho fold 4
â”œâ”€â”€ best_model_fold_5.pth          # Best model cho fold 5
â”œâ”€â”€ cross_validation_results.png   # Visualization plots
â””â”€â”€ cross_validation_summary.json  # Detailed results
```

### Performance Metrics
- **Mean Accuracy**: X.XXXX Â± X.XXXX
- **Mean F1-Score**: X.XXXX Â± X.XXXX  
- **Mean Kappa**: X.XXXX Â± X.XXXX
- **Training Time**: X minutes per fold

### Visualizations
- Performance metrics across folds
- Box plots of metrics distribution  
- Training history curves
- Confusion matrices
- Training time comparison

## ğŸ¯ Model Architecture

```
CBraMod Model:
â”œâ”€â”€ Backbone (FROZEN)
â”‚   â”œâ”€â”€ Patch Embedding: 19 channels â†’ 200 dims
â”‚   â”œâ”€â”€ Transformer Encoder: 12 layers, 8 heads
â”‚   â””â”€â”€ Pretrained weights: 4,883,800 parameters
â””â”€â”€ Classifier (TRAINABLE)
    â”œâ”€â”€ AdaptiveAvgPool2d
    â”œâ”€â”€ Flatten  
    â””â”€â”€ Linear: 200 â†’ 2 classes
    â””â”€â”€ Total trainable: 402 parameters
```

## âš™ï¸ Configuration

### Model Parameters
```python
class Args:
    num_subjects = 121
    window_size = 200
    encoding_dims = [256, 256]
    ff_hidden_size = 256
    attn_heads = 8
    depth = 12
    dropout_rate = 0.4
    freeze_backbone = True      # ğŸ§Š Frozen for speed
    num_of_classes = 2          # ADHD vs Control
    use_pretrained_weights = True
```

### Training Parameters
- **Optimizer**: Adam (lr=0.001)
- **Loss Function**: CrossEntropyLoss
- **Batch Size**: 64
- **Epochs per Fold**: 5
- **K-Fold Splits**: 5 (stratified)

## Dataset Format

File `datasets/adhdata.csv` cáº§n cÃ³ format:
```
Fp1,Fp2,F3,F4,C3,C4,P3,P4,O1,O2,F7,F8,T7,T8,P7,P8,Fz,Cz,Pz,Class,ID
261.0,402.0,16.0,...,ADHD,v10p
121.0,191.0,-94.0,...,Control,v11p
...
```

- 19 cá»™t EEG channels: Fp1, Fp2, F3, F4, C3, C4, P3, P4, O1, O2, F7, F8, T7, T8, P7, P8, Fz, Cz, Pz
- Class: "ADHD" hoáº·c "Control" (sáº½ Ä‘Æ°á»£c encode thÃ nh 0, 1)
- ID: identifier cho má»—i sample

## ğŸ”§ Troubleshooting

### Common Issues

1. **CUDA Out of Memory**:
   ```python
   # Giáº£m batch size trong notebook
   batch_size = 32  # thay vÃ¬ 64
   ```

2. **DataLoader Error on Windows**:
   ```python
   # ÄÃ£ fix: num_workers=0 cho Windows compatibility
   train_loader = DataLoader(..., num_workers=0)
   ```

3. **Module Not Found**:
   ```bash
   # CÃ i einops
   conda install -c conda-forge einops
   # hoáº·c
   pip install einops
   ```

### Memory Requirements
- **Full Dataset (2.1M)**: ~0.15 GB RAM
- **Sample Dataset (10K)**: ~0.001 GB RAM
- **GPU Memory**: ~2-4 GB (depending on batch size)

## ğŸ“ˆ Expected Performance

### With Full Dataset (2.1M samples):
- **Accuracy**: 65-75% (vs 50% random baseline)
- **Training Time**: 15-20 minutes total
- **Memory Usage**: 0.15 GB RAM

### With Sample Dataset (10K samples):
- **Accuracy**: 55-65% 
- **Training Time**: 5-10 minutes total
- **Memory Usage**: ~0.001 GB RAM

## ğŸ¯ Tips for Best Results

1. **Use Full Dataset** cho maximum accuracy
2. **Monitor â­ NEW markers** Ä‘á»ƒ track best epochs
3. **Check class balance** trong má»—i fold
4. **Save results** trÆ°á»›c khi close notebook
5. **Adjust sample size** náº¿u cáº§n balance speed vs accuracy

## ğŸ“ Files Structure

```
E:\Workspace\CBraMod\
â”œâ”€â”€ kfold_adhd_finetune.ipynb     # Main notebook
â”œâ”€â”€ ADHD_README.md                # This file
â”œâ”€â”€ datasets/
â”‚   â””â”€â”€ adhdata.csv              # ADHD dataset (2.1M samples)
â”œâ”€â”€ models/
â”‚   â””â”€â”€ model_for_adhd.py        # CBraMod model for ADHD
â”œâ”€â”€ pretrained_weights/
â”‚   â””â”€â”€ pretrained_weights.pth   # Pretrained CBraMod weights
â””â”€â”€ saved_models/                # Output models vÃ  results
    â”œâ”€â”€ best_model_fold_*.pth
    â”œâ”€â”€ cross_validation_results.png
    â””â”€â”€ cross_validation_summary.json
```

## ğŸš€ Quick Start

```bash
# 1. Clone repo vÃ  navigate
cd E:\Workspace\CBraMod

# 2. Install dependencies  
conda install -c conda-forge einops

# 3. Open notebook
jupyter notebook kfold_adhd_finetune.ipynb

# 4. Chá»n dataset size (cell 3)
USE_FULL_DATASET = True  # cho best results

# 5. Run all cells tuáº§n tá»±
# 6. Wait ~15-20 minutes
# 7. Check results trong saved_models/
```

## ğŸ“ Support

Náº¿u gáº·p váº¥n Ä‘á»:
1. Check requirements Ä‘Ã£ install Ä‘á»§ chÆ°a
2. Verify dataset path: `./datasets/adhdata.csv`
3. Check GPU memory vá»›i `nvidia-smi`
4. Restart kernel náº¿u cáº§n

---

**Happy Fine-tuning with K-Fold Cross Validation! ğŸ‰**